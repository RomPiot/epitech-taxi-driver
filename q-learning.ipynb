{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+---------+\n",
                        "|R: |\u001b[43m \u001b[0m: :G|\n",
                        "| : | : : |\n",
                        "| : : : : |\n",
                        "| | : | : |\n",
                        "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
                        "+---------+\n",
                        "\n",
                        "Total possible actions: 6\n",
                        "Total states: 500\n"
                    ]
                }
            ],
            "source": [
                "import gym\n",
                "import numpy as np\n",
                "import pandas as pd \n",
                "from random import random\n",
                "import time\n",
                "import csv\n",
                "\n",
                "\n",
                "def open_dataset(file_name):\n",
                "    return pd.read_csv(filepath_or_buffer=file_name, delimiter=\",\", encoding=\"utf-8\", header=0)\n",
                "\n",
                "\n",
                "env = gym.make(\"Taxi-v3\")\n",
                "env.reset()\n",
                "env.render()\n",
                "\n",
                "actions = env.action_space.n\n",
                "print(f\"Total possible actions: {actions}\")\n",
                "\n",
                "states = env.observation_space.n\n",
                "print(f\"Total states: {states}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[[0. 0. 0. 0. 0. 0.]\n",
                        " [0. 0. 0. 0. 0. 0.]\n",
                        " [0. 0. 0. 0. 0. 0.]\n",
                        " ...\n",
                        " [0. 0. 0. 0. 0. 0.]\n",
                        " [0. 0. 0. 0. 0. 0.]\n",
                        " [0. 0. 0. 0. 0. 0.]]\n"
                    ]
                }
            ],
            "source": [
                "# Create Q table of rewards defined to 0, for each actions on each step\n",
                "q_table = np.zeros((states, actions))\n",
                "print(q_table)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "def play_to_taxi(environment, q_table, epsilon=1.0, epsilon_min=0.01, epsilon_max=1.0, learning_rate=0.02, reward_discount_rate=0.618, decay_rate = 0.01, episodes=100000, max_steps=99, training=True, reset=False, show_print=True):\n",
                "    t = time.process_time()\n",
                "    training_time = 0\n",
                "    won_episode = 0\n",
                "    total_steps = []\n",
                "    total_rewards = []\n",
                "\n",
                "    if reset == True:\n",
                "        q_table = np.zeros((states, actions))\n",
                "\n",
                "    done = False\n",
                "    \n",
                "    if show_print:\n",
                "        if training:\n",
                "            print('')\n",
                "            print('---------------------- TRAINING ----------------------')\n",
                "        else:\n",
                "            print('')\n",
                "            print('---------------------- TESTING ----------------------')\n",
                "\n",
                "    for episode in range(episodes):\n",
                "        environment.reset()\n",
                "        state = 0\n",
                "        total_episode_rewards = 0\n",
                "        total_episode_steps = 0\n",
                "        done = False\n",
                "        \n",
                "        for step in range(max_steps):\n",
                "            # epsilon-greedy\n",
                "            if training == True and random() < epsilon: # Exploration\n",
                "                action = environment.action_space.sample() # get random action\n",
                "                \n",
                "            else: # Exploitation\n",
                "                possibilities = q_table[state,:] # possibilities from current state\n",
                "                action = np.argmax(possibilities) # get the best direction depending on the reward value\n",
                "            \n",
                "            # Move to direction\n",
                "            next_state, reward, done, info = environment.step(action)\n",
                "\n",
                "            if training == True:\n",
                "                # Update Q table with value function\n",
                "                # V(s) = V(s) + (lr x (V(s') - V(s)))\n",
                "                # state_value = state_value + alpha x (reward + gamma x next_state_value - state_value)\n",
                "                q_table[state, action] = q_table[state, action] + learning_rate * (reward + reward_discount_rate * np.max(q_table[next_state, :]) - q_table[state, action])\n",
                "\n",
                "\n",
                "            state = next_state\n",
                "            total_episode_steps = step + 1\n",
                "            \n",
                "            # Update statistics\n",
                "            total_episode_rewards += reward\n",
                "\n",
                "\n",
                "            if done:\n",
                "                total_rewards.append(total_episode_rewards)\n",
                "                won_episode += 1\n",
                "                # print(f\"Score: {total_episode_rewards}\")\n",
                "                break\n",
                "\n",
                "\n",
                "        # game is ended  \n",
                "        total_steps.append(total_episode_steps)\n",
                "\n",
                "         # epsilon decay to maintain trade-off between exploration-exploitation\n",
                "        epsilon = epsilon_min + (epsilon_max - epsilon_min) * np.exp(-decay_rate * episode)\n",
                "                \n",
                "    \n",
                "    # print(f'Total rewards : {total_rewards}')\n",
                "    # print(f'Total steps : {total_steps}')\n",
                "    \n",
                "    avg_rewards = round(sum(total_rewards) / len(total_rewards), 2)\n",
                "    avg_steps = round(sum(total_steps) / len(total_steps), 2)\n",
                "    won_rate = round(won_episode / episodes * 100, 2)\n",
                "    training_time = round(time.process_time() - t, 2)\n",
                "\n",
                "    if show_print:\n",
                "        print(f'Total episodes : {episodes}')\n",
                "        print(f'Average rewards : {avg_rewards}')\n",
                "        print(f'Average steps : {avg_steps}')\n",
                "        print(f'Won episode : {won_episode} ({won_rate}%)')\n",
                "\n",
                "        if training == True:\n",
                "            print(f'Training time : {training_time} seconds')\n",
                "\n",
                "    return {\n",
                "        'q_table': q_table,\n",
                "        'avg_rewards': avg_rewards,\n",
                "        'avg_steps': avg_steps,\n",
                "    }\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "epsilon_rate = 1.0              # Exploration vs exploitation\n",
                "learning_rate = 0.7             # Learning rate\n",
                "epsilon_max = 1.0               # Exploration probability at the start\n",
                "epsilon_min = 0.01              # Minimum exploration probability\n",
                "reward_discount_rate = 0.618    # Discounting rate for rewards\n",
                "decay_rate = 0.01 \n",
                "\n",
                "nb_episodes_list = [i for i in range(40001, 50001) if (i)/10000 % 1 == 0]\n",
                "learning_rate_list = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
                "reward_discount_rate_list = [0.9,  0.786, 0.618, 0.5, 0.382, 0.2, 0.1]\n",
                "decay_rate_list = [0.01, 0.03, 0.05, 0.07, 0.1]\n",
                "\n",
                "best_avg_rewards = 0\n",
                "best_nb_episodes = 0\n",
                "best_learning_rate = 0\n",
                "best_reward_discount_rate = 0\n",
                "best_decay_rate = 0\n",
                "\n",
                "for nb_episodes in nb_episodes_list:\n",
                "    for learning_rate in learning_rate_list:\n",
                "        for reward_discount_rate in reward_discount_rate_list:\n",
                "            for decay_rate in decay_rate_list:\n",
                "                training_result = play_to_taxi(environment=env, \n",
                "                        q_table=q_table, \n",
                "                        epsilon=epsilon_rate, \n",
                "                        epsilon_min=epsilon_min,\n",
                "                        epsilon_max=epsilon_max,\n",
                "                        learning_rate=learning_rate, \n",
                "                        episodes=nb_episodes,\n",
                "                        reward_discount_rate=reward_discount_rate, \n",
                "                        decay_rate=decay_rate,\n",
                "                        training=True,\n",
                "                        show_print=False,\n",
                "                        reset=True\n",
                "                        )\n",
                "\n",
                "                test_result = play_to_taxi(environment=env, \n",
                "                        q_table=training_result['q_table'], \n",
                "                        epsilon=epsilon_rate, \n",
                "                        epsilon_min=epsilon_min,\n",
                "                        epsilon_max=epsilon_max,\n",
                "                        learning_rate=learning_rate, \n",
                "                        episodes=nb_episodes,\n",
                "                        reward_discount_rate=reward_discount_rate, \n",
                "                        decay_rate=decay_rate,\n",
                "                        training=False,\n",
                "                        show_print=False,\n",
                "                        reset=False\n",
                "                        )\n",
                "\n",
                "                with open('q-learning.csv', 'a', encoding='UTF8', newline='') as f:\n",
                "                    writer = csv.writer(f)\n",
                "                    writer.writerow([nb_episodes, epsilon_rate, epsilon_min, epsilon_max, learning_rate, reward_discount_rate, decay_rate, test_result['avg_rewards'], test_result['avg_steps']])\n",
                "                "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>nb_episodes</th>\n",
                            "      <th>epsilon_rate</th>\n",
                            "      <th>epsilon_min</th>\n",
                            "      <th>epsilon_max</th>\n",
                            "      <th>learning_rate</th>\n",
                            "      <th>reward_discount_rate</th>\n",
                            "      <th>decay_rate</th>\n",
                            "      <th>avg_rewards</th>\n",
                            "      <th>avg_steps</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>10000</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>0.01</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>0.8</td>\n",
                            "      <td>0.786</td>\n",
                            "      <td>0.07</td>\n",
                            "      <td>7.08</td>\n",
                            "      <td>13.92</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>10000</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>0.01</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>0.5</td>\n",
                            "      <td>0.200</td>\n",
                            "      <td>0.07</td>\n",
                            "      <td>7.07</td>\n",
                            "      <td>13.93</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>10000</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>0.01</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>0.9</td>\n",
                            "      <td>0.382</td>\n",
                            "      <td>0.01</td>\n",
                            "      <td>7.07</td>\n",
                            "      <td>13.93</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>10000</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>0.01</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>0.5</td>\n",
                            "      <td>0.500</td>\n",
                            "      <td>0.10</td>\n",
                            "      <td>7.07</td>\n",
                            "      <td>13.93</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>30000</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>0.01</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>0.7</td>\n",
                            "      <td>0.618</td>\n",
                            "      <td>0.10</td>\n",
                            "      <td>7.07</td>\n",
                            "      <td>13.93</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "   nb_episodes  epsilon_rate  epsilon_min  epsilon_max  learning_rate  \\\n",
                            "0        10000           1.0         0.01          1.0            0.8   \n",
                            "1        10000           1.0         0.01          1.0            0.5   \n",
                            "2        10000           1.0         0.01          1.0            0.9   \n",
                            "3        10000           1.0         0.01          1.0            0.5   \n",
                            "4        30000           1.0         0.01          1.0            0.7   \n",
                            "\n",
                            "   reward_discount_rate  decay_rate  avg_rewards  avg_steps  \n",
                            "0                 0.786        0.07         7.08      13.92  \n",
                            "1                 0.200        0.07         7.07      13.93  \n",
                            "2                 0.382        0.01         7.07      13.93  \n",
                            "3                 0.500        0.10         7.07      13.93  \n",
                            "4                 0.618        0.10         7.07      13.93  "
                        ]
                    },
                    "execution_count": 16,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "qlearning_data = open_dataset('q-learning.csv')\n",
                "qlearning_data = qlearning_data.sort_values(\n",
                "    by=['avg_rewards'], ascending=[False]).reset_index(drop=True)\n",
                "qlearning_data.head(5)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "---------------------- TRAINING ----------------------\n",
                        "Total episodes : 10000\n",
                        "Average rewards : 5.74\n",
                        "Average steps : 15.96\n",
                        "Won episode : 9869 (98.69%)\n",
                        "Training time : 3.97 seconds\n",
                        "\n",
                        "---------------------- TESTING ----------------------\n",
                        "Total episodes : 10000\n",
                        "Average rewards : 7.02\n",
                        "Average steps : 13.98\n",
                        "Won episode : 10000 (100.0%)\n"
                    ]
                }
            ],
            "source": [
                "nb_episodes = 10000             # Number of games to be played\n",
                "epsilon_rate = 1.0              # Exploration vs exploitation\n",
                "epsilon_min = 0.01              # Minimum exploration probability\n",
                "epsilon_max = 1.0               # Exploration probability at the start\n",
                "learning_rate = 0.8             # Learning rate\n",
                "reward_discount_rate = 0.786    # Discounting rate for rewards\n",
                "decay_rate = 0.07\n",
                "\n",
                "result_training = play_to_taxi(environment=env, \n",
                "             q_table=q_table, \n",
                "             epsilon=epsilon_rate, \n",
                "             epsilon_min=epsilon_min,\n",
                "             epsilon_max=epsilon_max,\n",
                "             learning_rate=learning_rate, \n",
                "             episodes=nb_episodes,\n",
                "             reward_discount_rate=reward_discount_rate, \n",
                "             decay_rate=decay_rate,\n",
                "             training=True,\n",
                "             reset=True)\n",
                "\n",
                "result_testing = play_to_taxi(environment=env, \n",
                "             q_table=result_training['q_table'], \n",
                "             epsilon=epsilon_rate, \n",
                "             epsilon_min=epsilon_min,\n",
                "             epsilon_max=epsilon_max,\n",
                "             learning_rate=learning_rate, \n",
                "             episodes=nb_episodes,\n",
                "             reward_discount_rate=reward_discount_rate, \n",
                "             decay_rate=decay_rate,\n",
                "             training=False)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "8cc97f30dc116ff925e94344d6c26b092429f59185c53817ffd568135f6f3797"
        },
        "kernelspec": {
            "display_name": "Python 3.8.10 ('env': venv)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.12"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}

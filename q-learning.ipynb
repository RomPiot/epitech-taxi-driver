{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "ename": "error",
                    "evalue": "No available video device",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
                        "\u001b[1;32m/home/romain/projects/epitech/t-aia-902-group-29/q-learning.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/romain/projects/epitech/t-aia-902-group-29/q-learning.ipynb#ch0000000vscode-remote?line=12'>13</a>\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m\"\u001b[39m\u001b[39mTaxi-v3\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/romain/projects/epitech/t-aia-902-group-29/q-learning.ipynb#ch0000000vscode-remote?line=13'>14</a>\u001b[0m env\u001b[39m.\u001b[39mreset()\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/romain/projects/epitech/t-aia-902-group-29/q-learning.ipynb#ch0000000vscode-remote?line=14'>15</a>\u001b[0m env\u001b[39m.\u001b[39;49mrender()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/romain/projects/epitech/t-aia-902-group-29/q-learning.ipynb#ch0000000vscode-remote?line=16'>17</a>\u001b[0m actions \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/romain/projects/epitech/t-aia-902-group-29/q-learning.ipynb#ch0000000vscode-remote?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTotal possible actions: \u001b[39m\u001b[39m{\u001b[39;00mactions\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
                        "File \u001b[0;32m~/projects/epitech/t-aia-902-group-29/back_django/.venv/lib/python3.8/site-packages/gym/core.py:328\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    327\u001b[0m     \u001b[39m\"\"\"Renders the environment with kwargs.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 328\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
                        "File \u001b[0;32m~/projects/epitech/t-aia-902-group-29/back_django/.venv/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py:51\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_disable_render_order_enforcing \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     47\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\n\u001b[1;32m     48\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m     )\n\u001b[0;32m---> 51\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
                        "File \u001b[0;32m~/projects/epitech/t-aia-902-group-29/back_django/.venv/lib/python3.8/site-packages/gym/envs/toy_text/taxi.py:238\u001b[0m, in \u001b[0;36mTaxiEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_render_text()\n\u001b[1;32m    237\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 238\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render_gui(mode)\n",
                        "File \u001b[0;32m~/projects/epitech/t-aia-902-group-29/back_django/.venv/lib/python3.8/site-packages/gym/envs/toy_text/taxi.py:252\u001b[0m, in \u001b[0;36mTaxiEnv._render_gui\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mset_caption(\u001b[39m\"\u001b[39m\u001b[39mTaxi\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    251\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 252\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow \u001b[39m=\u001b[39m pygame\u001b[39m.\u001b[39;49mdisplay\u001b[39m.\u001b[39;49mset_mode(WINDOW_SIZE)\n\u001b[1;32m    253\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# \"rgb_array\"\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow \u001b[39m=\u001b[39m pygame\u001b[39m.\u001b[39mSurface(WINDOW_SIZE)\n",
                        "\u001b[0;31merror\u001b[0m: No available video device"
                    ]
                }
            ],
            "source": [
                "import gym\n",
                "import numpy as np\n",
                "import pandas as pd \n",
                "from random import random\n",
                "import time\n",
                "import csv\n",
                "\n",
                "\n",
                "def open_dataset(file_name):\n",
                "    return pd.read_csv(filepath_or_buffer=file_name, delimiter=\",\", encoding=\"utf-8\", header=0)\n",
                "\n",
                "\n",
                "env = gym.make(\"Taxi-v3\")\n",
                "env.reset()\n",
                "env.render()\n",
                "\n",
                "actions = env.action_space.n\n",
                "print(f\"Total possible actions: {actions}\")\n",
                "\n",
                "states = env.observation_space.n\n",
                "print(f\"Total states: {states}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[[0. 0. 0. 0. 0. 0.]\n",
                        " [0. 0. 0. 0. 0. 0.]\n",
                        " [0. 0. 0. 0. 0. 0.]\n",
                        " ...\n",
                        " [0. 0. 0. 0. 0. 0.]\n",
                        " [0. 0. 0. 0. 0. 0.]\n",
                        " [0. 0. 0. 0. 0. 0.]]\n"
                    ]
                }
            ],
            "source": [
                "# Create Q table of rewards defined to 0, for each actions on each step\n",
                "q_table = np.zeros((states, actions))\n",
                "print(q_table)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "def play_to_taxi(environment, q_table, epsilon=1.0, epsilon_min=0.01, epsilon_max=1.0, learning_rate=0.02, reward_discount_rate=0.618, decay_rate = 0.01, episodes=100000, max_steps=99, training=True, reset=False, show_print=True):\n",
                "    t = time.process_time()\n",
                "    training_time = 0\n",
                "    won_episode = 0\n",
                "    total_steps = []\n",
                "    total_rewards = []\n",
                "\n",
                "    if reset == True:\n",
                "        q_table = np.zeros((states, actions))\n",
                "\n",
                "    done = False\n",
                "    \n",
                "    if show_print:\n",
                "        if training:\n",
                "            print('')\n",
                "            print('---------------------- TRAINING ----------------------')\n",
                "        else:\n",
                "            print('')\n",
                "            print('---------------------- TESTING ----------------------')\n",
                "\n",
                "    for episode in range(episodes):\n",
                "        environment.reset()\n",
                "        state = 0\n",
                "        total_episode_rewards = 0\n",
                "        total_episode_steps = 0\n",
                "        done = False\n",
                "        \n",
                "        for step in range(max_steps):\n",
                "            # epsilon-greedy\n",
                "            if training == True and random() < epsilon: # Exploration\n",
                "                action = environment.action_space.sample() # get random action\n",
                "                \n",
                "            else: # Exploitation\n",
                "                possibilities = q_table[state,:] # possibilities from current state\n",
                "                action = np.argmax(possibilities) # get the best direction depending on the reward value\n",
                "            \n",
                "            # Move to direction\n",
                "            next_state, reward, done, info = environment.step(action)\n",
                "\n",
                "            if training == True:\n",
                "                # Update Q table with value function\n",
                "                # V(s) = V(s) + (lr x (V(s') - V(s)))\n",
                "                # state_value = state_value + alpha x (reward + gamma x next_state_value - state_value)\n",
                "                q_table[state, action] = q_table[state, action] + learning_rate * (reward + reward_discount_rate * np.max(q_table[next_state, :]) - q_table[state, action])\n",
                "\n",
                "\n",
                "            state = next_state\n",
                "            total_episode_steps = step + 1\n",
                "            \n",
                "            # Update statistics\n",
                "            total_episode_rewards += reward\n",
                "\n",
                "\n",
                "            if done:\n",
                "                total_rewards.append(total_episode_rewards)\n",
                "                won_episode += 1\n",
                "                # print(f\"Score: {total_episode_rewards}\")\n",
                "                break\n",
                "\n",
                "\n",
                "        # game is ended  \n",
                "        total_steps.append(total_episode_steps)\n",
                "\n",
                "         # epsilon decay to maintain trade-off between exploration-exploitation\n",
                "        epsilon = epsilon_min + (epsilon_max - epsilon_min) * np.exp(-decay_rate * episode)\n",
                "                \n",
                "    \n",
                "    # print(f'Total rewards : {total_rewards}')\n",
                "    # print(f'Total steps : {total_steps}')\n",
                "    \n",
                "    avg_rewards = round(sum(total_rewards) / len(total_rewards), 2)\n",
                "    avg_steps = round(sum(total_steps) / len(total_steps), 2)\n",
                "    won_rate = round(won_episode / episodes * 100, 2)\n",
                "    training_time = round(time.process_time() - t, 2)\n",
                "\n",
                "    if show_print:\n",
                "        print(f'Total episodes : {episodes}')\n",
                "        print(f'Average rewards : {avg_rewards}')\n",
                "        print(f'Average steps : {avg_steps}')\n",
                "        print(f'Won episode : {won_episode} ({won_rate}%)')\n",
                "\n",
                "        if training == True:\n",
                "            print(f'Training time : {training_time} seconds')\n",
                "\n",
                "    return {\n",
                "        'q_table': q_table,\n",
                "        'avg_rewards': avg_rewards,\n",
                "        'avg_steps': avg_steps,\n",
                "    }\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "epsilon_rate = 1.0              # Exploration vs exploitation\n",
                "learning_rate = 0.7             # Learning rate\n",
                "epsilon_max = 1.0               # Exploration probability at the start\n",
                "epsilon_min = 0.01              # Minimum exploration probability\n",
                "reward_discount_rate = 0.618    # Discounting rate for rewards\n",
                "decay_rate = 0.01 \n",
                "\n",
                "nb_episodes_list = [i for i in range(1, 50001) if (i)/10000 % 1 == 0]\n",
                "learning_rate_list = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
                "reward_discount_rate_list = [0.9,  0.786, 0.618, 0.5, 0.382, 0.2, 0.1]\n",
                "decay_rate_list = [0.01, 0.03, 0.05, 0.07, 0.1]\n",
                "\n",
                "best_avg_rewards = 0\n",
                "best_nb_episodes = 0\n",
                "best_learning_rate = 0\n",
                "best_reward_discount_rate = 0\n",
                "best_decay_rate = 0\n",
                "\n",
                "for nb_episodes in nb_episodes_list:\n",
                "    for learning_rate in learning_rate_list:\n",
                "        for reward_discount_rate in reward_discount_rate_list:\n",
                "            for decay_rate in decay_rate_list:\n",
                "                training_result = play_to_taxi(environment=env, \n",
                "                        q_table=q_table, \n",
                "                        epsilon=epsilon_rate, \n",
                "                        epsilon_min=epsilon_min,\n",
                "                        epsilon_max=epsilon_max,\n",
                "                        learning_rate=learning_rate, \n",
                "                        episodes=nb_episodes,\n",
                "                        reward_discount_rate=reward_discount_rate, \n",
                "                        decay_rate=decay_rate,\n",
                "                        training=True,\n",
                "                        show_print=False,\n",
                "                        reset=True\n",
                "                        )\n",
                "\n",
                "                test_result = play_to_taxi(environment=env, \n",
                "                        q_table=training_result['q_table'], \n",
                "                        epsilon=epsilon_rate, \n",
                "                        epsilon_min=epsilon_min,\n",
                "                        epsilon_max=epsilon_max,\n",
                "                        learning_rate=learning_rate, \n",
                "                        episodes=nb_episodes,\n",
                "                        reward_discount_rate=reward_discount_rate, \n",
                "                        decay_rate=decay_rate,\n",
                "                        training=False,\n",
                "                        show_print=False,\n",
                "                        reset=False\n",
                "                        )\n",
                "\n",
                "                with open('q-learning.csv', 'a', encoding='UTF8', newline='') as f:\n",
                "                    writer = csv.writer(f)\n",
                "                    writer.writerow([nb_episodes, epsilon_rate, epsilon_min, epsilon_max, learning_rate, reward_discount_rate, decay_rate, test_result['avg_rewards'], test_result['avg_steps']])\n",
                "                "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>nb_episodes</th>\n",
                            "      <th>epsilon_rate</th>\n",
                            "      <th>epsilon_min</th>\n",
                            "      <th>epsilon_max</th>\n",
                            "      <th>learning_rate</th>\n",
                            "      <th>reward_discount_rate</th>\n",
                            "      <th>decay_rate</th>\n",
                            "      <th>avg_rewards</th>\n",
                            "      <th>avg_steps</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>10000</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>0.01</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>0.8</td>\n",
                            "      <td>0.786</td>\n",
                            "      <td>0.07</td>\n",
                            "      <td>7.08</td>\n",
                            "      <td>13.92</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>10000</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>0.01</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>0.5</td>\n",
                            "      <td>0.200</td>\n",
                            "      <td>0.07</td>\n",
                            "      <td>7.07</td>\n",
                            "      <td>13.93</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>10000</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>0.01</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>0.9</td>\n",
                            "      <td>0.382</td>\n",
                            "      <td>0.01</td>\n",
                            "      <td>7.07</td>\n",
                            "      <td>13.93</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>10000</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>0.01</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>0.5</td>\n",
                            "      <td>0.500</td>\n",
                            "      <td>0.10</td>\n",
                            "      <td>7.07</td>\n",
                            "      <td>13.93</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>30000</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>0.01</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>0.7</td>\n",
                            "      <td>0.618</td>\n",
                            "      <td>0.10</td>\n",
                            "      <td>7.07</td>\n",
                            "      <td>13.93</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "   nb_episodes  epsilon_rate  epsilon_min  epsilon_max  learning_rate  \\\n",
                            "0        10000           1.0         0.01          1.0            0.8   \n",
                            "1        10000           1.0         0.01          1.0            0.5   \n",
                            "2        10000           1.0         0.01          1.0            0.9   \n",
                            "3        10000           1.0         0.01          1.0            0.5   \n",
                            "4        30000           1.0         0.01          1.0            0.7   \n",
                            "\n",
                            "   reward_discount_rate  decay_rate  avg_rewards  avg_steps  \n",
                            "0                 0.786        0.07         7.08      13.92  \n",
                            "1                 0.200        0.07         7.07      13.93  \n",
                            "2                 0.382        0.01         7.07      13.93  \n",
                            "3                 0.500        0.10         7.07      13.93  \n",
                            "4                 0.618        0.10         7.07      13.93  "
                        ]
                    },
                    "execution_count": 16,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "qlearning_data = open_dataset('q-learning.csv')\n",
                "qlearning_data = qlearning_data.sort_values(\n",
                "    by=['avg_rewards'], ascending=[False]).reset_index(drop=True)\n",
                "qlearning_data.head(5)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "---------------------- TRAINING ----------------------\n",
                        "Total episodes : 10000\n",
                        "Average rewards : 5.74\n",
                        "Average steps : 15.96\n",
                        "Won episode : 9869 (98.69%)\n",
                        "Training time : 3.97 seconds\n",
                        "\n",
                        "---------------------- TESTING ----------------------\n",
                        "Total episodes : 10000\n",
                        "Average rewards : 7.02\n",
                        "Average steps : 13.98\n",
                        "Won episode : 10000 (100.0%)\n"
                    ]
                }
            ],
            "source": [
                "nb_episodes = 10000             # Number of games to be played\n",
                "epsilon_rate = 1.0              # Exploration vs exploitation\n",
                "epsilon_min = 0.01              # Minimum exploration probability\n",
                "epsilon_max = 1.0               # Exploration probability at the start\n",
                "learning_rate = 0.8             # Learning rate\n",
                "reward_discount_rate = 0.786    # Discounting rate for rewards\n",
                "decay_rate = 0.07\n",
                "\n",
                "result_training = play_to_taxi(environment=env, \n",
                "             q_table=q_table, \n",
                "             epsilon=epsilon_rate, \n",
                "             epsilon_min=epsilon_min,\n",
                "             epsilon_max=epsilon_max,\n",
                "             learning_rate=learning_rate, \n",
                "             episodes=nb_episodes,\n",
                "             reward_discount_rate=reward_discount_rate, \n",
                "             decay_rate=decay_rate,\n",
                "             training=True,\n",
                "             reset=True)\n",
                "\n",
                "result_testing = play_to_taxi(environment=env, \n",
                "             q_table=result_training['q_table'], \n",
                "             epsilon=epsilon_rate, \n",
                "             epsilon_min=epsilon_min,\n",
                "             epsilon_max=epsilon_max,\n",
                "             learning_rate=learning_rate, \n",
                "             episodes=nb_episodes,\n",
                "             reward_discount_rate=reward_discount_rate, \n",
                "             decay_rate=decay_rate,\n",
                "             training=False)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "---------------------- Parameters ----------------------\n",
                        "Episode iterations : 10000\n",
                        "Epsilon rate : 1.0\n",
                        "Epsilon min : 0.01\n",
                        "Epsilon max : 1.0\n",
                        "Learning rate : 0.8\n",
                        "Reward discount rate : 0.786\n",
                        "Decay rate : 0.07\n",
                        "\n",
                        "---------------------- TRAINING ----------------------\n",
                        "Total episodes : 10000\n",
                        "Average rewards : 5.73\n",
                        "Average steps : 15.93\n",
                        "Won episode : 9872 (98.72%)\n",
                        "Training time : 3.75 seconds\n",
                        "\n",
                        "---------------------- TESTING ----------------------\n",
                        "Total episodes : 10000\n",
                        "Average rewards : 6.77\n",
                        "Average steps : 14.23\n",
                        "Won episode : 10000 (100.0%)\n"
                    ]
                }
            ],
            "source": [
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.8.10 ('.venv': venv)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "e522e05d946741890393366e21d91dbd32eee82fb1102c65afea95555a5b1745"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}

{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+---------+\n",
                        "|\u001b[43mR\u001b[0m: | : :G|\n",
                        "| : | : : |\n",
                        "| : : : : |\n",
                        "| | : | : |\n",
                        "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
                        "+---------+\n",
                        "\n",
                        "Total possible actions: 6\n",
                        "Total states: 500\n"
                    ]
                }
            ],
            "source": [
                "import gym\n",
                "import numpy as np\n",
                "import pandas as pd \n",
                "from random import random\n",
                "import time\n",
                "import csv\n",
                "\n",
                "env = gym.make(\"Taxi-v3\")\n",
                "env.reset()\n",
                "env.render()\n",
                "\n",
                "actions = env.action_space.n\n",
                "print(f\"Total possible actions: {actions}\")\n",
                "\n",
                "states = env.observation_space.n\n",
                "print(f\"Total states: {states}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[[0. 0. 0. 0. 0. 0.]\n",
                        " [0. 0. 0. 0. 0. 0.]\n",
                        " [0. 0. 0. 0. 0. 0.]\n",
                        " ...\n",
                        " [0. 0. 0. 0. 0. 0.]\n",
                        " [0. 0. 0. 0. 0. 0.]\n",
                        " [0. 0. 0. 0. 0. 0.]]\n"
                    ]
                }
            ],
            "source": [
                "# Create Q table of rewards defined to 0, for each actions on each step\n",
                "q_table = np.zeros((states, actions))\n",
                "print(q_table)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "def play_to_taxi(environment, q_table, epsilon=1.0, epsilon_min=0.01, epsilon_max=1.0, learning_rate=0.02, reward_discount_rate=0.618, decay_rate = 0.01, episodes=100000, max_steps=99, training=True, reset=False, show_print=True):\n",
                "    t = time.process_time()\n",
                "    training_time = 0\n",
                "    won_episode = 0\n",
                "    # total_rewards = 0\n",
                "    # total_steps = 0\n",
                "    # avg_rewards = 0\n",
                "    total_steps = []\n",
                "    total_rewards = []\n",
                "\n",
                "    if show_print:\n",
                "        if training:\n",
                "            print('')\n",
                "            print('---------------------- TRAINING ----------------------')\n",
                "        else:\n",
                "            print('')\n",
                "            print('---------------------- TESTING ----------------------')\n",
                "    \n",
                "\n",
                "    if reset == True:\n",
                "        q_table = np.zeros((states, actions))\n",
                "\n",
                "    done = False\n",
                "    \n",
                "    for episode in range(episodes):\n",
                "        environment.reset()\n",
                "        state = 0\n",
                "        total_episode_rewards = 0\n",
                "        total_episode_steps = 0\n",
                "        done = False\n",
                "        \n",
                "        for step in range(max_steps):\n",
                "            # epsilon-greedy\n",
                "            if training == True and random() < epsilon: # Exploration\n",
                "                action = environment.action_space.sample() # get random action\n",
                "                \n",
                "            else: # Exploitation\n",
                "                possibilities = q_table[state,:] # possibilities from current state\n",
                "                action = np.argmax(possibilities) # get the best direction depending on the reward value\n",
                "            \n",
                "            # Move to direction\n",
                "            next_state, reward, done, info = environment.step(action)\n",
                "\n",
                "            if training == True:\n",
                "                # Update Q table with value function\n",
                "                # V(s) = V(s) + (lr x (V(s') - V(s)))\n",
                "                # state_value = state_value + alpha x (reward + gamma x next_state_value - state_value)\n",
                "                q_table[state, action] = q_table[state, action] + learning_rate * (reward + reward_discount_rate * np.max(q_table[next_state, :]) - q_table[state, action])\n",
                "\n",
                "\n",
                "            state = next_state\n",
                "            total_episode_steps = step + 1\n",
                "            \n",
                "            # Update statistics\n",
                "            total_episode_rewards += reward\n",
                "\n",
                "\n",
                "            if done:\n",
                "                total_rewards.append(total_episode_rewards)\n",
                "                won_episode += 1\n",
                "                # print(f\"Score: {total_episode_rewards}\")\n",
                "                break\n",
                "\n",
                "\n",
                "        # game is ended  \n",
                "        # total_rewards += total_episode_rewards\n",
                "        # total_steps += total_episode_steps\n",
                "        total_steps.append(total_episode_steps)\n",
                "\n",
                "         # epsilon decay to maintain trade-off between exploration-exploitation\n",
                "        epsilon = epsilon_min + (epsilon_max - epsilon_min) * np.exp(-decay_rate * episode)\n",
                "                \n",
                "    \n",
                "    # print(f'Total rewards : {total_rewards}')\n",
                "    # print(f'Total steps : {total_steps}')\n",
                "    \n",
                "    avg_rewards = round(sum(total_rewards) / len(total_rewards), 2)\n",
                "    avg_steps = round(sum(total_steps) / len(total_steps), 2)\n",
                "    won_rate = round(won_episode / episodes * 100, 2)\n",
                "    training_time = round(time.process_time() - t, 2)\n",
                "\n",
                "    if show_print:\n",
                "        print(f'Total episodes : {episodes}')\n",
                "        print(f'Average rewards : {avg_rewards}')\n",
                "        print(f'Average steps : {avg_steps}')\n",
                "        print(f'Won episode : {won_episode} ({won_rate}%)')\n",
                "\n",
                "        if training == True:\n",
                "            print(f'Training time : {training_time} seconds')\n",
                "\n",
                "    return {\n",
                "        'avg_rewards': avg_rewards,\n",
                "        'avg_steps': avg_steps,\n",
                "    }\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "---------------------- TRAINING ----------------------\n",
                        "Total episodes : 10000\n",
                        "Average rewards : 5.39\n",
                        "Average steps : 16.35\n",
                        "Won episode : 9826 (98.26%)\n",
                        "Training time : 3.86 seconds\n",
                        "\n",
                        "---------------------- TESTING ----------------------\n",
                        "Total episodes : 10000\n",
                        "Average rewards : 7.03\n",
                        "Average steps : 13.97\n",
                        "Won episode : 10000 (100.0%)\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "100.0"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "nb_episodes = 10000             # Number of games to be played\n",
                "epsilon_rate = 1.0              # Exploration vs exploitation\n",
                "learning_rate = 0.7             # Learning rate\n",
                "epsilon_max = 1.0               # Exploration probability at the start\n",
                "epsilon_min = 0.01              # Minimum exploration probability\n",
                "reward_discount_rate = 0.618    # Discounting rate for rewards\n",
                "decay_rate = 0.01 \n",
                "\n",
                "play_to_taxi(environment=env, \n",
                "             q_table=q_table, \n",
                "             epsilon=epsilon_rate, \n",
                "             epsilon_min=epsilon_min,\n",
                "             epsilon_max=epsilon_max,\n",
                "             learning_rate=learning_rate, \n",
                "             episodes=nb_episodes,\n",
                "             reward_discount_rate=reward_discount_rate, \n",
                "             decay_rate=decay_rate,\n",
                "             training=True)\n",
                "\n",
                "play_to_taxi(environment=env, \n",
                "             q_table=q_table, \n",
                "             epsilon=epsilon_rate, \n",
                "             epsilon_min=epsilon_min,\n",
                "             epsilon_max=epsilon_max,\n",
                "             learning_rate=learning_rate, \n",
                "             episodes=nb_episodes,\n",
                "             reward_discount_rate=reward_discount_rate, \n",
                "             decay_rate=decay_rate,\n",
                "             training=False)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "---------------------- TRAINING ----------------------\n",
                        "Total episodes : 50000\n",
                        "Average rewards : 6.23\n",
                        "Average steps : 14.29\n",
                        "Won episode : 49981 (99.96%)\n",
                        "Training time : 16.78 seconds\n",
                        "\n",
                        "---------------------- TESTING ----------------------\n",
                        "Total episodes : 50000\n",
                        "Average rewards : 7.04\n",
                        "Average steps : 13.96\n",
                        "Won episode : 50000 (100.0%)\n"
                    ]
                }
            ],
            "source": [
                "nb_episodes = 50000             # Number of games to be played\n",
                "epsilon_rate = 1.0              # Exploration vs exploitation\n",
                "learning_rate = 0.7             # learning rate\n",
                "epsilon_max = 1.0               # Exploration probability at the start\n",
                "epsilon_min = 0.01              # Minimum exploration probability\n",
                "reward_discount_rate = 0.618    # Discounting rate for rewards\n",
                "decay_rate = 0.01 \n",
                "\n",
                "print('---------------------- TRAINING ----------------------')\n",
                "\n",
                "play_to_taxi(environment=env, \n",
                "             q_table=q_table, \n",
                "             epsilon=epsilon_rate, \n",
                "             epsilon_min=epsilon_min,\n",
                "             epsilon_max=epsilon_max,\n",
                "             learning_rate=learning_rate, \n",
                "             episodes=nb_episodes,\n",
                "             reward_discount_rate=reward_discount_rate, \n",
                "             decay_rate=decay_rate,\n",
                "             training=True)\n",
                "\n",
                "print('')\n",
                "print('---------------------- TESTING ----------------------')\n",
                "play_to_taxi(environment=env, \n",
                "             q_table=q_table, \n",
                "             epsilon=epsilon_rate, \n",
                "             epsilon_min=epsilon_min,\n",
                "             epsilon_max=epsilon_max,\n",
                "             learning_rate=learning_rate, \n",
                "             episodes=nb_episodes,\n",
                "             reward_discount_rate=reward_discount_rate, \n",
                "             decay_rate=decay_rate,\n",
                "             training=False)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "---------------------- TRAINING ----------------------\n",
                        "Total episodes : 30000\n",
                        "Average rewards : 6.06\n",
                        "Average steps : 14.35\n",
                        "Won episode : 29982 (99.94%)\n",
                        "Training time : 10.08 seconds\n",
                        "\n",
                        "---------------------- TESTING ----------------------\n",
                        "Total episodes : 30000\n",
                        "Average rewards : 6.84\n",
                        "Average steps : 14.16\n",
                        "Won episode : 30000 (100.0%)\n"
                    ]
                }
            ],
            "source": [
                "nb_episodes = 30000             # Number of games to be played\n",
                "epsilon_rate = 1.0              # Exploration vs exploitation\n",
                "learning_rate = 0.7             # Learning rate\n",
                "epsilon_max = 1.0               # Exploration probability at the start\n",
                "epsilon_min = 0.01              # Minimum exploration probability\n",
                "reward_discount_rate = 0.618    # Discounting rate for rewards\n",
                "decay_rate = 0.01 \n",
                "\n",
                "print('---------------------- TRAINING ----------------------')\n",
                "\n",
                "play_to_taxi(environment=env, \n",
                "             q_table=q_table, \n",
                "             epsilon=epsilon_rate, \n",
                "             epsilon_min=epsilon_min,\n",
                "             epsilon_max=epsilon_max,\n",
                "             learning_rate=learning_rate, \n",
                "             episodes=nb_episodes,\n",
                "             reward_discount_rate=reward_discount_rate, \n",
                "             decay_rate=decay_rate,\n",
                "             training=True)\n",
                "\n",
                "print('')\n",
                "print('---------------------- TESTING ----------------------')\n",
                "play_to_taxi(environment=env, \n",
                "             q_table=q_table, \n",
                "             epsilon=epsilon_rate, \n",
                "             epsilon_min=epsilon_min,\n",
                "             epsilon_max=epsilon_max,\n",
                "             learning_rate=learning_rate, \n",
                "             episodes=nb_episodes,\n",
                "             reward_discount_rate=reward_discount_rate, \n",
                "             decay_rate=decay_rate,\n",
                "             training=False)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "epsilon_rate = 1.0              # Exploration vs exploitation\n",
                "learning_rate = 0.7             # Learning rate\n",
                "epsilon_max = 1.0               # Exploration probability at the start\n",
                "epsilon_min = 0.01              # Minimum exploration probability\n",
                "reward_discount_rate = 0.618    # Discounting rate for rewards\n",
                "decay_rate = 0.01 \n",
                "\n",
                "nb_episodes_list = [i for i in range(20001, 70001) if (i)/10000 % 1 == 0]\n",
                "learning_rate_list = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
                "reward_discount_rate_list = [0.9,  0.786, 0.618, 0.5, 0.382, 0.2, 0.1]\n",
                "decay_rate_list = [0.01, 0.03, 0.05, 0.07, 0.1]\n",
                "\n",
                "best_avg_rewards = 0\n",
                "best_nb_episodes = 0\n",
                "best_learning_rate = 0\n",
                "best_reward_discount_rate = 0\n",
                "best_decay_rate = 0\n",
                "\n",
                "for nb_episodes in nb_episodes_list:\n",
                "    for learning_rate in learning_rate_list:\n",
                "        for reward_discount_rate in reward_discount_rate_list:\n",
                "            for decay_rate in decay_rate_list:\n",
                "                # print('')\n",
                "                # print(nb_episodes)\n",
                "                training_result = play_to_taxi(environment=env, \n",
                "                        q_table=q_table, \n",
                "                        epsilon=epsilon_rate, \n",
                "                        epsilon_min=epsilon_min,\n",
                "                        epsilon_max=epsilon_max,\n",
                "                        learning_rate=learning_rate, \n",
                "                        episodes=nb_episodes,\n",
                "                        reward_discount_rate=reward_discount_rate, \n",
                "                        decay_rate=decay_rate,\n",
                "                        training=True,\n",
                "                        show_print=False\n",
                "                        )\n",
                "\n",
                "                test_result = play_to_taxi(environment=env, \n",
                "                        q_table=q_table, \n",
                "                        epsilon=epsilon_rate, \n",
                "                        epsilon_min=epsilon_min,\n",
                "                        epsilon_max=epsilon_max,\n",
                "                        learning_rate=learning_rate, \n",
                "                        episodes=nb_episodes,\n",
                "                        reward_discount_rate=reward_discount_rate, \n",
                "                        decay_rate=decay_rate,\n",
                "                        training=False,\n",
                "                        show_print=False\n",
                "                        )\n",
                "\n",
                "                with open('q-learning.csv', 'a', encoding='UTF8', newline='') as f:\n",
                "                    writer = csv.writer(f)\n",
                "                    writer.writerow([nb_episodes, epsilon_rate, epsilon_min, epsilon_max, learning_rate, reward_discount_rate, decay_rate, test_result['avg_rewards'], test_result['avg_steps']])\n",
                "                \n",
                "                if test_result['avg_rewards'] > best_avg_rewards:\n",
                "                    best_avg_rewards = test_result['avg_rewards']\n",
                "                    best_nb_episodes = nb_episodes\n",
                "                    best_learning_rate = learning_rate\n",
                "                    best_reward_discount_rate = reward_discount_rate\n",
                "                    best_decay_rate = decay_rate\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[30000, 40000, 50000, 60000, 70000]\n"
                    ]
                }
            ],
            "source": [
                "print(nb_episodes_list)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "10000\n",
                        "11.23\n",
                        "0.6\n",
                        "0.1\n",
                        "0.03\n"
                    ]
                }
            ],
            "source": [
                "print(best_nb_episodes)\n",
                "print(best_avg_rewards)\n",
                "print(best_learning_rate)\n",
                "print(best_reward_discount_rate)\n",
                "print(best_decay_rate)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "---------------------- TRAINING ----------------------\n",
                        "Total episodes : 10000\n",
                        "Average rewards : 7.76\n",
                        "Average steps : 94.0\n",
                        "Won episode : 576 (5.76%)\n",
                        "Training time : 20.64 seconds\n",
                        "\n",
                        "---------------------- TESTING ----------------------\n",
                        "Total episodes : 10000\n",
                        "Average rewards : 8.73\n",
                        "Average steps : 92.0\n",
                        "Won episode : 807 (8.07%)\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "{'avg_rewards': 8.73, 'avg_steps': 92.0}"
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "nb_episodes = 10000             # Number of games to be played\n",
                "epsilon_rate = 1.0              # Exploration vs exploitation\n",
                "learning_rate = 0.6             # Learning rate\n",
                "epsilon_max = 1.0               # Exploration probability at the start\n",
                "epsilon_min = 0.01              # Minimum exploration probability\n",
                "reward_discount_rate = 0.1      # Discounting rate for rewards\n",
                "decay_rate = 0.03 \n",
                "\n",
                "play_to_taxi(environment=env, \n",
                "             q_table=q_table, \n",
                "             epsilon=epsilon_rate, \n",
                "             epsilon_min=epsilon_min,\n",
                "             epsilon_max=epsilon_max,\n",
                "             learning_rate=learning_rate, \n",
                "             episodes=nb_episodes,\n",
                "             reward_discount_rate=reward_discount_rate, \n",
                "             decay_rate=decay_rate,\n",
                "             training=True)\n",
                "\n",
                "play_to_taxi(environment=env, \n",
                "             q_table=q_table, \n",
                "             epsilon=epsilon_rate, \n",
                "             epsilon_min=epsilon_min,\n",
                "             epsilon_max=epsilon_max,\n",
                "             learning_rate=learning_rate, \n",
                "             episodes=nb_episodes,\n",
                "             reward_discount_rate=reward_discount_rate, \n",
                "             decay_rate=decay_rate,\n",
                "             training=False)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "header = ['nb_episodes', 'epsilon_rate', 'epsilon_min', 'epsilon_max', 'learning_rate', 'reward_discount_rate', 'decay_rate', 'avg_rewards', 'avg_steps']\n",
                "data = ['Afghanistan', 652090, 'AF', 'AFG']\n",
                "\n",
                "\n",
                "with open('q-learning.csv', 'a', encoding='UTF8', newline='') as f:\n",
                "    writer = csv.writer(f)\n",
                "    # writer.writerow(header)\n",
                "    writer.writerow(data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "8cc97f30dc116ff925e94344d6c26b092429f59185c53817ffd568135f6f3797"
        },
        "kernelspec": {
            "display_name": "Python 3.8.10 ('env': venv)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.12"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}

{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+---------+\n",
                        "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
                        "| : | : : |\n",
                        "| : : : : |\n",
                        "| | : | : |\n",
                        "|Y|\u001b[43m \u001b[0m: |B: |\n",
                        "+---------+\n",
                        "\n",
                        "Total possible actions: 6\n",
                        "Total states: 500\n"
                    ]
                }
            ],
            "source": [
                "import gym\n",
                "import numpy as np\n",
                "import pandas as pd \n",
                "from random import random\n",
                "import time\n",
                "from tqdm import tqdm\n",
                "\n",
                "env = gym.make(\"Taxi-v3\")\n",
                "env.reset()\n",
                "env.render()\n",
                "\n",
                "# Info actions\n",
                "# Down = 1\n",
                "# Right = 2\n",
                "# Up = 3\n",
                "# Left = 4\n",
                "\n",
                "actions = env.action_space.n\n",
                "print(f\"Total possible actions: {actions}\")\n",
                "\n",
                "states = env.observation_space.n\n",
                "print(f\"Total states: {states}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[[0. 0. 0. 0. 0. 0.]\n",
                        " [0. 0. 0. 0. 0. 0.]\n",
                        " [0. 0. 0. 0. 0. 0.]\n",
                        " ...\n",
                        " [0. 0. 0. 0. 0. 0.]\n",
                        " [0. 0. 0. 0. 0. 0.]\n",
                        " [0. 0. 0. 0. 0. 0.]]\n"
                    ]
                }
            ],
            "source": [
                "# Create Q table of rewards defined to 0, for each actions on each step\n",
                "q_table = np.zeros((states, actions))\n",
                "print(q_table)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "def play_to_taxi(environment, q_table, epsilon=1.0, epsilon_min=0.01, epsilon_max=1.0, learning_rate=0.02, reward_discount_rate=0.618, decay_rate = 0.01, episodes=100000, training=True, reset=False):\n",
                "    t = time.process_time()\n",
                "    training_time = 0\n",
                "    won_episode = 0\n",
                "    total_rewards = 0\n",
                "    total_steps = 0\n",
                "    avg_rewards = 0\n",
                "    avg_steps = []\n",
                "\n",
                "    if reset == True:\n",
                "        q_table = np.zeros((states, actions))\n",
                "\n",
                "    done = False\n",
                "    \n",
                "    for episode in tqdm(range(episodes)):\n",
                "        environment.reset()\n",
                "        state = 0\n",
                "        total_reward = 0\n",
                "        steps = 0\n",
                "\n",
                "        while not done:\n",
                "            # epsilon-greedy\n",
                "            if training == True and random() < epsilon: # Exploration\n",
                "                action = environment.action_space.sample() # get random action\n",
                "                \n",
                "            else: # Exploitation\n",
                "                possibilities = q_table[state,:] # possibilities from current state\n",
                "                action = np.argmax(possibilities) # get the best direction depending on the reward value\n",
                "            \n",
                "            # Move to direction\n",
                "            next_state, reward, done, info = environment.step(action)\n",
                "\n",
                "            if training == True:\n",
                "                # Update Q table with value function\n",
                "                # V(s) = V(s) + (lr x (V(s') - V(s)))\n",
                "                # state_value = state_value + alpha x (reward + gamma x next_state_value - state_value)\n",
                "                q_table[state, action] = q_table[state, action] + learning_rate * (reward + reward_discount_rate * np.max(q_table[next_state, :]) - q_table[state, action])\n",
                "\n",
                "                # epsilon decay to maintain trade-off between exploration-exploitation\n",
                "                epsilon = epsilon_min + (epsilon_max - epsilon_min) * np.exp(-decay_rate * episode)\n",
                "                \n",
                "            state = next_state\n",
                "            \n",
                "            # Update statistics\n",
                "            steps += 1\n",
                "            total_reward += reward\n",
                "\n",
                "            if state == 15:\n",
                "                won_episode += 1 \n",
                "\n",
                "\n",
                "        # game is over    \n",
                "        # avg_rewards += total_reward\n",
                "        \n",
                "        avg_steps.append(steps)\n",
                "\n",
                "        total_rewards += total_reward\n",
                "        total_steps += steps\n",
                "             \n",
                "        # if training == True:\n",
                "        #     if episode % 10000 == 0:\n",
                "        #         avg_rewards /= 10000\n",
                "        #         avg_steps /= 10000\n",
                "        #         print(f\"Episode {episode} : average steps = {avg_steps}, reward = {avg_rewards}\")\n",
                "        #         avg_rewards = 0\n",
                "        #         # avg_steps = 0\n",
                "                \n",
                "        #     if episode % 100000 == 0:\n",
                "        #         learning_rate = learning_rate * 1.1\n",
                "\n",
                "\n",
                "    print(f'Total episodes : {episodes}')\n",
                "    print(f'Total rewards : {total_rewards}')\n",
                "    print(f'Total steps : {total_steps}')\n",
                "    \n",
                "    avg_rewards = total_rewards / episodes\n",
                "    avg_steps = sum(avg_steps) / len(avg_steps)\n",
                "\n",
                "    training_time = time.process_time() - t\n",
                "    \n",
                "    print(f'Average rewards : {avg_rewards}')\n",
                "    print(f'Average steps : {avg_steps}')\n",
                "    print(f'Won episode : {won_episode} ({won_episode / episodes * 100}%)')\n",
                "    if training == True:\n",
                "        print(f'Training time : {training_time} seconds')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "---------------------- TRAINING ----------------------\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 50000/50000 [00:00<00:00, 116822.96it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Total episodes : 50000\n",
                        "Total rewards : -758\n",
                        "Total steps : 200\n",
                        "Average rewards : -0.01516\n",
                        "Average steps : 0.004\n",
                        "Won episode : 0 (0.0%)\n",
                        "Training time : 0.421875 seconds\n",
                        "\n",
                        "---------------------- TESTING ----------------------\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 50000/50000 [00:00<00:00, 109649.27it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Total episodes : 50000\n",
                        "Total rewards : -200\n",
                        "Total steps : 200\n",
                        "Average rewards : -0.004\n",
                        "Average steps : 0.004\n",
                        "Won episode : 0 (0.0%)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "nb_episodes = 50000 # number of games to be played\n",
                "epsilon_rate = 1.0 # exploration vs exploitation\n",
                "learning_rate = 0.7 # \n",
                "epsilon_max = 1.0            # Exploration probability at the start\n",
                "epsilon_min = 0.01           # Minimum exploration probability\n",
                "reward_discount_rate = 0.618                # Discounting rate for rewards\n",
                "decay_rate = 0.01 \n",
                "\n",
                "print('---------------------- TRAINING ----------------------')\n",
                "\n",
                "play_to_taxi(environment=env, \n",
                "             q_table=q_table, \n",
                "             epsilon=epsilon_rate, \n",
                "             epsilon_min=epsilon_min,\n",
                "             epsilon_max=epsilon_max,\n",
                "             learning_rate=learning_rate, \n",
                "             episodes=nb_episodes,\n",
                "             reward_discount_rate=reward_discount_rate, \n",
                "             decay_rate=decay_rate,\n",
                "             training=True)\n",
                "\n",
                "print('')\n",
                "print('---------------------- TESTING ----------------------')\n",
                "play_to_taxi(environment=env, \n",
                "             q_table=q_table, \n",
                "             epsilon=epsilon_rate, \n",
                "             epsilon_min=epsilon_min,\n",
                "             epsilon_max=epsilon_max,\n",
                "             learning_rate=learning_rate, \n",
                "             episodes=nb_episodes,\n",
                "             reward_discount_rate=reward_discount_rate, \n",
                "             decay_rate=decay_rate,\n",
                "             training=False)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "8cc97f30dc116ff925e94344d6c26b092429f59185c53817ffd568135f6f3797"
        },
        "kernelspec": {
            "display_name": "Python 3.8.10 ('env': venv)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.12"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}

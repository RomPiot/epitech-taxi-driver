{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import gym\n",
                "import numpy as np\n",
                "import pandas as pd \n",
                "from random import random\n",
                "import time\n",
                "import csv\n",
                "\n",
                "\n",
                "def open_dataset(file_name):\n",
                "    return pd.read_csv(filepath_or_buffer=file_name, delimiter=\",\", encoding=\"utf-8\", header=0)\n",
                "\n",
                "\n",
                "env = gym.make(\"Taxi-v3\")\n",
                "env.reset()\n",
                "env.render()\n",
                "\n",
                "actions = env.action_space.n\n",
                "print(f\"Total possible actions: {actions}\")\n",
                "\n",
                "states = env.observation_space.n\n",
                "print(f\"Total states: {states}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create Q table of rewards defined to 0, for each actions on each step\n",
                "q_table = np.zeros((states, actions))\n",
                "print(q_table)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def play_to_taxi(environment, q_table, epsilon=1.0, epsilon_min=0.01, epsilon_max=1.0, learning_rate=0.02, reward_discount_rate=0.618, decay_rate = 0.01, episodes=100000, max_steps=99, training=True, reset=False, show_print=True):\n",
                "    t = time.process_time()\n",
                "    training_time = 0\n",
                "    won_episode = 0\n",
                "    total_steps = []\n",
                "    total_rewards = []\n",
                "\n",
                "    if reset == True:\n",
                "        q_table = np.zeros((states, actions))\n",
                "\n",
                "    done = False\n",
                "    \n",
                "    if show_print:\n",
                "        if training:\n",
                "            print('')\n",
                "            print('---------------------- TRAINING ----------------------')\n",
                "        else:\n",
                "            print('')\n",
                "            print('---------------------- TESTING ----------------------')\n",
                "\n",
                "    for episode in range(episodes):\n",
                "        environment.reset()\n",
                "        state = 0\n",
                "        total_episode_rewards = 0\n",
                "        total_episode_steps = 0\n",
                "        done = False\n",
                "        \n",
                "        for step in range(max_steps):\n",
                "            # epsilon-greedy\n",
                "            if training == True and random() < epsilon: # Exploration\n",
                "                action = environment.action_space.sample() # get random action\n",
                "                \n",
                "            else: # Exploitation\n",
                "                possibilities = q_table[state,:] # possibilities from current state\n",
                "                action = np.argmax(possibilities) # get the best direction depending on the reward value\n",
                "            \n",
                "            # Move to direction\n",
                "            next_state, reward, done, info = environment.step(action)\n",
                "\n",
                "            if training == True:\n",
                "                # Update Q table with value function\n",
                "                # V(s) = V(s) + (lr x (V(s') - V(s)))\n",
                "                # state_value = state_value + alpha x (reward + gamma x next_state_value - state_value)\n",
                "                q_table[state, action] = q_table[state, action] + learning_rate * (reward + reward_discount_rate * np.max(q_table[next_state, :]) - q_table[state, action])\n",
                "\n",
                "\n",
                "            state = next_state\n",
                "            total_episode_steps = step + 1\n",
                "            \n",
                "            # Update statistics\n",
                "            total_episode_rewards += reward\n",
                "\n",
                "\n",
                "            if done:\n",
                "                total_rewards.append(total_episode_rewards)\n",
                "                won_episode += 1\n",
                "                # print(f\"Score: {total_episode_rewards}\")\n",
                "                break\n",
                "\n",
                "\n",
                "        # game is ended  \n",
                "        total_steps.append(total_episode_steps)\n",
                "\n",
                "         # epsilon decay to maintain trade-off between exploration-exploitation\n",
                "        epsilon = epsilon_min + (epsilon_max - epsilon_min) * np.exp(-decay_rate * episode)\n",
                "                \n",
                "    \n",
                "    # print(f'Total rewards : {total_rewards}')\n",
                "    # print(f'Total steps : {total_steps}')\n",
                "    \n",
                "    avg_rewards = round(sum(total_rewards) / len(total_rewards), 2)\n",
                "    avg_steps = round(sum(total_steps) / len(total_steps), 2)\n",
                "    won_rate = round(won_episode / episodes * 100, 2)\n",
                "    training_time = round(time.process_time() - t, 2)\n",
                "\n",
                "    if show_print:\n",
                "        print(f'Total episodes : {episodes}')\n",
                "        print(f'Average rewards : {avg_rewards}')\n",
                "        print(f'Average steps : {avg_steps}')\n",
                "        print(f'Won episode : {won_episode} ({won_rate}%)')\n",
                "\n",
                "        if training == True:\n",
                "            print(f'Training time : {training_time} seconds')\n",
                "\n",
                "    return {\n",
                "        'q_table': q_table,\n",
                "        'avg_rewards': avg_rewards,\n",
                "        'avg_steps': avg_steps,\n",
                "    }\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "epsilon_rate = 1.0              # Exploration vs exploitation\n",
                "learning_rate = 0.7             # Learning rate\n",
                "epsilon_max = 1.0               # Exploration probability at the start\n",
                "epsilon_min = 0.01              # Minimum exploration probability\n",
                "reward_discount_rate = 0.618    # Discounting rate for rewards\n",
                "decay_rate = 0.01 \n",
                "\n",
                "nb_episodes_list = [i for i in range(1, 50001) if (i)/10000 % 1 == 0]\n",
                "learning_rate_list = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
                "reward_discount_rate_list = [0.9,  0.786, 0.618, 0.5, 0.382, 0.2, 0.1]\n",
                "decay_rate_list = [0.01, 0.03, 0.05, 0.07, 0.1]\n",
                "\n",
                "best_avg_rewards = 0\n",
                "best_nb_episodes = 0\n",
                "best_learning_rate = 0\n",
                "best_reward_discount_rate = 0\n",
                "best_decay_rate = 0\n",
                "\n",
                "for nb_episodes in nb_episodes_list:\n",
                "    for learning_rate in learning_rate_list:\n",
                "        for reward_discount_rate in reward_discount_rate_list:\n",
                "            for decay_rate in decay_rate_list:\n",
                "                training_result = play_to_taxi(environment=env, \n",
                "                        q_table=q_table, \n",
                "                        epsilon=epsilon_rate, \n",
                "                        epsilon_min=epsilon_min,\n",
                "                        epsilon_max=epsilon_max,\n",
                "                        learning_rate=learning_rate, \n",
                "                        episodes=nb_episodes,\n",
                "                        reward_discount_rate=reward_discount_rate, \n",
                "                        decay_rate=decay_rate,\n",
                "                        training=True,\n",
                "                        show_print=False,\n",
                "                        reset=True\n",
                "                        )\n",
                "\n",
                "                test_result = play_to_taxi(environment=env, \n",
                "                        q_table=training_result['q_table'], \n",
                "                        epsilon=epsilon_rate, \n",
                "                        epsilon_min=epsilon_min,\n",
                "                        epsilon_max=epsilon_max,\n",
                "                        learning_rate=learning_rate, \n",
                "                        episodes=nb_episodes,\n",
                "                        reward_discount_rate=reward_discount_rate, \n",
                "                        decay_rate=decay_rate,\n",
                "                        training=False,\n",
                "                        show_print=False,\n",
                "                        reset=False\n",
                "                        )\n",
                "\n",
                "                with open('q-learning.csv', 'a', encoding='UTF8', newline='') as f:\n",
                "                    writer = csv.writer(f)\n",
                "                    writer.writerow([nb_episodes, epsilon_rate, epsilon_min, epsilon_max, learning_rate, reward_discount_rate, decay_rate, test_result['avg_rewards'], test_result['avg_steps']])\n",
                "                "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "qlearning_data = open_dataset('q-learning.csv')\n",
                "qlearning_data = qlearning_data.sort_values(\n",
                "    by=['avg_rewards'], ascending=[False]).reset_index(drop=True)\n",
                "qlearning_data.head(5)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "nb_episodes = 10000             # Number of games to be played\n",
                "epsilon_rate = 1.0              # Exploration vs exploitation\n",
                "epsilon_min = 0.01              # Minimum exploration probability\n",
                "epsilon_max = 1.0               # Exploration probability at the start\n",
                "learning_rate = 0.8             # Learning rate\n",
                "reward_discount_rate = 0.786    # Discounting rate for rewards\n",
                "decay_rate = 0.07\n",
                "\n",
                "result_training = play_to_taxi(environment=env, \n",
                "             q_table=q_table, \n",
                "             epsilon=epsilon_rate, \n",
                "             epsilon_min=epsilon_min,\n",
                "             epsilon_max=epsilon_max,\n",
                "             learning_rate=learning_rate, \n",
                "             episodes=nb_episodes,\n",
                "             reward_discount_rate=reward_discount_rate, \n",
                "             decay_rate=decay_rate,\n",
                "             training=True,\n",
                "             reset=True)\n",
                "\n",
                "result_testing = play_to_taxi(environment=env, \n",
                "             q_table=result_training['q_table'], \n",
                "             epsilon=epsilon_rate, \n",
                "             epsilon_min=epsilon_min,\n",
                "             epsilon_max=epsilon_max,\n",
                "             learning_rate=learning_rate, \n",
                "             episodes=nb_episodes,\n",
                "             reward_discount_rate=reward_discount_rate, \n",
                "             decay_rate=decay_rate,\n",
                "             training=False)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.8.10 ('.venv': venv)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "e522e05d946741890393366e21d91dbd32eee82fb1102c65afea95555a5b1745"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}

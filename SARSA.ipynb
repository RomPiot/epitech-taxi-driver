{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : : : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "Action Space 6\n",
      "State Space 500\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import time, pickle, os, csv, random\n",
    "\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "\n",
    "def open_dataset(file_name):\n",
    "    return pd.read_csv(filepath_or_buffer=file_name, delimiter=\",\", encoding=\"utf-8\", header=0)\n",
    "\n",
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "actions = env.action_space.n\n",
    "states = env.observation_space.n\n",
    "\n",
    "print(\"Action Space {}\".format(actions))\n",
    "print(\"State Space {}\".format(states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 428, -1, False)],\n",
       " 1: [(1.0, 228, -1, False)],\n",
       " 2: [(1.0, 348, -1, False)],\n",
       " 3: [(1.0, 328, -1, False)],\n",
       " 4: [(1.0, 328, -10, False)],\n",
       " 5: [(1.0, 328, -10, False)]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# {action: [(probability, nextstate, reward, done)]}\n",
    "\n",
    "env.P[328]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Create Q table of rewards defined to 0, for each actions on each step\n",
    "q_table = np.zeros((states, actions))\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy(q_table, s, epsilon=0.1, training=False):\n",
    "    '''\n",
    "    Epsilon greedy policy\n",
    "    '''\n",
    "    if training == True and np.random.uniform(0,1) < epsilon:\n",
    "        # Choose a random action\n",
    "        return np.random.randint(q_table.shape[1])\n",
    "    else:\n",
    "        # Choose the action of a greedy policy\n",
    "        return greedy(q_table, s)\n",
    "\n",
    "\n",
    "def greedy(q_table, s):\n",
    "    '''\n",
    "    Greedy policy\n",
    "    return the index corresponding to the maximum action-state value\n",
    "    '''\n",
    "    return np.argmax(q_table[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_taxi_with_sarsa(environment, q_table, epsilon=1.0, epsilon_min=0.01, epsilon_max=1.0, learning_rate=0.02, reward_discount_rate=0.618, decay_rate = 0.01, episodes=100000, max_steps=99, training=True, reset=False, show_print=True):\n",
    "    t = time.process_time()\n",
    "    training_time = 0\n",
    "    won_episode = 0\n",
    "    total_steps = []\n",
    "    total_rewards = []\n",
    "\n",
    "    if episodes > 0:\n",
    "        if reset == True:\n",
    "            q_table = np.zeros((states, actions))\n",
    "\n",
    "        done = False\n",
    "    \n",
    "        if show_print:\n",
    "            if training:\n",
    "                print('')\n",
    "                print('---------------------- TRAINING ----------------------')\n",
    "            else:\n",
    "                print('')\n",
    "                print('---------------------- TESTING ----------------------')\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            environment.reset()\n",
    "            state = 0\n",
    "            total_episode_rewards = 0\n",
    "            total_episode_steps = 0\n",
    "            done = False\n",
    "            \n",
    "            for step in range(max_steps):\n",
    "                # epsilon-greedy\n",
    "                \n",
    "                action = eps_greedy(q_table, state, epsilon, training)\n",
    "                \n",
    "                # Move to direction\n",
    "                next_state, reward, done, info = environment.step(action)\n",
    "                next_action = eps_greedy(q_table, next_state, epsilon, training)\n",
    "\n",
    "                if training == True:\n",
    "                    # Update Q table with value function\n",
    "                    # V(s) = V(s) + (lr x (V(s') - V(s)))\n",
    "                    # V(s) = V(s) + (lr x (V(s') - V(s)))\n",
    "                    # state_value = state_value + alpha x (reward + gamma x next_state_value - state_value)                \n",
    "                    q_table[state, action] = q_table[state][action] + learning_rate*(reward + reward_discount_rate*(q_table[next_state][next_action]) - q_table[state][action])\n",
    "\n",
    "\n",
    "                state = next_state\n",
    "                total_episode_steps = step + 1\n",
    "                \n",
    "                # Update statistics\n",
    "                total_episode_rewards += reward\n",
    "\n",
    "\n",
    "                if done:\n",
    "                    total_rewards.append(total_episode_rewards)\n",
    "                    won_episode += 1\n",
    "                    # print(f\"Score: {total_episode_rewards}\")\n",
    "                    break\n",
    "\n",
    "\n",
    "            # game is ended  \n",
    "            total_steps.append(total_episode_steps)\n",
    "\n",
    "            # epsilon decay to maintain trade-off between exploration-exploitation\n",
    "            epsilon = epsilon_min + (epsilon_max - epsilon_min) * np.exp(-decay_rate * episode)\n",
    "\n",
    "            total_time = time.process_time() - t\n",
    "            avg_rewards = round(sum(total_rewards) / len(total_rewards) if len(total_rewards) > 0 else 0, 2)\n",
    "            avg_steps = round(sum(total_steps) / len(total_steps) if len(total_steps) > 0 else 0, 2)\n",
    "            won_rate = round(won_episode / episodes * 100 if won_episode > 0 else 0, 2)\n",
    "            duration = round(total_time if total_time > 0 else 0, 2)\n",
    "\n",
    "            if show_print:\n",
    "                print(f'Total episodes : {episodes}')\n",
    "                print(f'Average rewards : {avg_rewards}')\n",
    "                print(f'Average steps : {avg_steps}')\n",
    "                print(f'Won episode : {won_episode} ({won_rate}%)')                          \n",
    "    \n",
    "    \n",
    "    avg_rewards = avg_rewards if \"avg_rewards\" in locals() else 0\n",
    "    avg_steps = avg_steps if \"avg_steps\" in locals() else 0\n",
    "    duration = duration if \"duration\" in locals() else 0\n",
    "    won_rate = won_rate if \"won_rate\" in locals() else 0    \n",
    "\n",
    "    \n",
    "\n",
    "    return {\n",
    "        'q_table': q_table,\n",
    "        'avg_rewards': avg_rewards,\n",
    "        'avg_steps': avg_steps,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_rate = 1.0              # Exploration vs exploitation\n",
    "learning_rate = 0.7             # Learning rate\n",
    "epsilon_max = 1.0               # Exploration probability at the start\n",
    "epsilon_min = 0.01              # Minimum exploration probability\n",
    "reward_discount_rate = 0.618    # Discounting rate for rewards\n",
    "decay_rate = 0.01 \n",
    "\n",
    "nb_episodes_list = [i for i in range(1, 50001) if (i)/10000 % 1 == 0]\n",
    "learning_rate_list = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "reward_discount_rate_list = [0.9,  0.786, 0.618, 0.5, 0.382, 0.2, 0.1]\n",
    "decay_rate_list = [0.01, 0.03, 0.05, 0.07, 0.1]\n",
    "\n",
    "best_avg_rewards = 0\n",
    "best_nb_episodes = 0\n",
    "best_learning_rate = 0\n",
    "best_reward_discount_rate = 0\n",
    "best_decay_rate = 0\n",
    "start = time.time()\n",
    "\n",
    "with open('SARSA.csv', 'a', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"nb_episodes\", \"epsilon_rate\", \"epsilon_min\", \"epsilon_max\", \"learning_rate\", \"reward_discount_rate\", \"decay_rate\", 'avg_rewards', 'avg_steps', \"duration\"])\n",
    "\n",
    "\n",
    "\n",
    "for nb_episodes in nb_episodes_list:\n",
    "    for learning_rate in learning_rate_list:\n",
    "        for reward_discount_rate in reward_discount_rate_list:\n",
    "            for decay_rate in decay_rate_list:\n",
    "                training_result = take_taxi_with_sarsa(environment=env, \n",
    "                        q_table=q_table, \n",
    "                        epsilon=epsilon_rate, \n",
    "                        epsilon_min=epsilon_min,\n",
    "                        epsilon_max=epsilon_max,\n",
    "                        learning_rate=learning_rate, \n",
    "                        episodes=nb_episodes,\n",
    "                        reward_discount_rate=reward_discount_rate, \n",
    "                        decay_rate=decay_rate,\n",
    "                        training=True,\n",
    "                        show_print=False,\n",
    "                        reset=True\n",
    "                        )\n",
    "\n",
    "                test_result = take_taxi_with_sarsa(environment=env, \n",
    "                        q_table=training_result['q_table'], \n",
    "                        epsilon=epsilon_rate, \n",
    "                        epsilon_min=epsilon_min,\n",
    "                        epsilon_max=epsilon_max,\n",
    "                        learning_rate=learning_rate, \n",
    "                        episodes=nb_episodes,\n",
    "                        reward_discount_rate=reward_discount_rate, \n",
    "                        decay_rate=decay_rate,\n",
    "                        training=False,\n",
    "                        show_print=False,\n",
    "                        reset=False\n",
    "                        )\n",
    "\n",
    "                with open('SARSA.csv', 'a', encoding='UTF8', newline='') as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow([nb_episodes, epsilon_rate, epsilon_min, epsilon_max, learning_rate, reward_discount_rate, decay_rate, test_result['avg_rewards'], test_result['avg_steps'], time.time() - start])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_data = open_dataset('SARSA.csv')\n",
    "sarsa_data = sarsa_data.sort_values(\n",
    "    by=['avg_rewards'], ascending=[False]).reset_index(drop=True)\n",
    "sarsa_data.head(5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a3b49de499be6a951e4f03c287c33fa79668f0b144552ff251d1b4e1156b6311"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
